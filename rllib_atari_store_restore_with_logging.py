# These tags allow extracting portions of this script on Anyscale.
# ws-template-imports-start
import gymnasium as gym
import logging
from pathlib import Path

from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.algorithms.algorithm import Algorithm
from ray.rllib.connectors.env_to_module.frame_stacking import FrameStackingEnvToModule
from ray.rllib.connectors.learner.frame_stacking import FrameStackingLearner
from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig
from ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack
from ray.rllib.utils.test_utils import add_rllib_example_script_args

from pprint import pprint

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)-30s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ws-template-imports-end
ENV_PREFIX = "ale_py:ALE"
ENV_ID = "Breakout-v5"

parser = add_rllib_example_script_args(
    default_reward=float("inf"),
    default_timesteps=3000000,
    default_iters=100000000000,
)
parser.set_defaults(
    env=f"{ENV_PREFIX}/{ENV_ID}"
)
# Use `parser` to add your own custom command line options to this script
# and (if needed) use their values to set up `config` below.
args = parser.parse_args()

NUM_LEARNERS = args.num_learners or 1
ENV = args.env

logger.info("="*100)
logger.info("RLlib Atari Training with Checkpoint Save/Restore Logging")
logger.info("="*100)
logger.info(f"Environment: {ENV}")
logger.info(f"Number of Learners: {NUM_LEARNERS}")


# These tags allow extracting portions of this script on Anyscale.
# ws-template-code-start
def _make_env_to_module_connector(env, spaces, device):
    return FrameStackingEnvToModule(num_frames=4)


def _make_learner_connector(input_observation_space, input_action_space):
    return FrameStackingLearner(num_frames=4)


# Create a custom Atari setup (w/o the usual RLlib-hard-coded framestacking in it).
# We would like our frame stacking connector to do this job.
def _env_creator(cfg):
    return wrap_atari_for_new_api_stack(
        gym.make(ENV, **cfg, render_mode="rgb_array"),
        # Perform frame-stacking through ConnectorV2 API.
        framestack=None,
    )


tune.register_env("env", _env_creator)
logger.info("âœ“ Environment registered with tune")

config = (
    PPOConfig()
    .environment(
        "env",
        env_config={
            "frameskip": 4,
            "full_action_space": False,
            "repeat_action_probability": 0.0,
        },
        clip_rewards=True,
    )
    .learners(
        # num_learners=0; ë¡œì»¬learner ì‚¬ìš©
        num_learners=0,
        num_gpus_per_learner=1
    )
    .env_runners(
        num_env_runners=8,
        num_envs_per_env_runner=4,
        num_cpus_per_env_runner=1,
        env_to_module_connector=_make_env_to_module_connector,
    )
    .training(
        learner_connector=_make_learner_connector,
        train_batch_size_per_learner=4000,
        minibatch_size=128,
        lambda_=0.95,
        kl_coeff=0.5,
        clip_param=0.1,
        vf_clip_param=10.0,
        entropy_coeff=0.01,
        num_epochs=10,
        lr=0.00015 * NUM_LEARNERS,
        grad_clip=100.0,
        grad_clip_by="global_norm",
    )
    .rl_module(
        model_config=DefaultModelConfig(
            conv_filters=[[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]],
            conv_activation="relu",
            head_fcnet_hiddens=[256],
            vf_share_layers=True,
        ),
    )
    # ì €ì¥ ì‹œ, í™˜ê²½ ì„¤ì •ì´ freeze ë˜ê¸°ì—, ì¶”í›„ í‰ê°€í•  ë•Œ ìˆ˜ì •ì´ ì–´ë ¤ì›€.
    # í•™ìŠµ ë•Œ ì„¤ì •í•´ë‘ëŠ” ê²ƒì´ ì¢‹ìŒ.
    .evaluation(
        evaluation_interval=None,               # í•„ìš” ì‹œ ë³€ê²½ ê°€ëŠ¥
        evaluation_num_env_runners=2,           # í‰ê°€ì— ì‚¬ìš©í•  EnvRunner ìˆ˜
        evaluation_duration=10,                 # í‰ê°€ ê¸¸ì´
        evaluation_duration_unit="episodes",    # 'episodes' ë˜ëŠ” 'timesteps'
        evaluation_config={
            "explore": False,                    # í‰ê°€ì—ì„œë§Œ íƒí—˜ ë¹„í™œì„±í™”
            "env": "env",
            "env_config": {
                "frameskip": 4,
                "full_action_space": False,
                "repeat_action_probability": 0.0,
            },
        },
    )
)

logger.info("-"*100)
logger.info("Configuration Summary:")
logger.info(f"  num_env_runners: 8")
logger.info(f"  num_envs_per_env_runner: 4")
logger.info(f"  train_batch_size_per_learner: 4000")
logger.info(f"  evaluation_num_env_runners: 2")
logger.info(f"  evaluation_duration: 10 episodes")
logger.info("-"*100)

logger.info("Building Algorithm...")
algo = config.build()
logger.info("âœ“ Algorithm built successfully")

logger.info("\n" + "="*100)
logger.info("Starting Training Loop (5 iterations)")
logger.info("="*100)

for i in range(5):
    logger.info(f"\n{'='*100}")
    logger.info(f"Training Iteration {i+1}/5")
    logger.info(f"{'='*100}")
    
    res = algo.train()
    
    try:
        # iteration ë‚´ì—ì„œ episodeê°€ ì•ˆ ëë‚¬ì„ ë•Œ KeyError ë°œìƒí•˜ê²Œ ë¨.
        # num_env_runners * num_envs_per_env_runner ì´ ì»¤ì§€ë©´ ë” ìì£¼ ë°œìƒ.
        # ë°œìƒì‹œí‚¤ê³  ì‹¶ì§€ ì•Šìœ¼ë©´ train_batch_size_per_learner ê°’ì„ í‚¤ìš°ë©´ ë¨
        ep_rew_mean = res["env_runners"]["episode_return_mean"]
        ep_len_mean = res["env_runners"].get("episode_len_mean", "N/A")
        num_episodes = res["env_runners"].get("num_episodes", "N/A")
        
        logger.info(f"âœ“ Iteration {i+1} completed successfully")
        logger.info(f"  Episode Return Mean: {ep_rew_mean:.2f}")
        logger.info(f"  Episode Length Mean: {ep_len_mean}")
        logger.info(f"  Number of Episodes: {num_episodes}")
        
        if "num_env_steps_sampled_lifetime" in res:
            logger.info(f"  Total Env Steps (Lifetime): {res['num_env_steps_sampled_lifetime']}")
            
    except KeyError as e:
        logger.warning(f"âš  Episode metrics not available yet (KeyError: {e})")
        logger.info("Full result structure:")
        pprint(res)

logger.info("\n" + "="*100)
logger.info("CHECKPOINT SAVE OPERATION")
logger.info("="*100)

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥
# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œë¥¼ PyArrow URI(ì˜ˆ: file:///..., s3://...)ë¡œ í•´ì„í•˜ë„ë¡ ë˜ì–´ ìˆì–´
# ë¡œì»¬ ê²½ë¡œë¼ë„ file:// ìŠ¤í‚´ì„ ë¶™ì´ê±°ë‚˜ ì ˆëŒ€ê²½ë¡œì˜ íŒŒì¼ URIë¡œ ë„˜ê²¨ì•¼ ë¨
target_dir = (Path.cwd() / "checkpoints" / ENV_ID).resolve()
target_dir.mkdir(parents=True, exist_ok=True)

logger.info(f"Checkpoint target directory: {target_dir}")
logger.info("Calling algo.save_to_path()...")

ckpt_path = algo.save_to_path(str(target_dir))

logger.info(f"âœ“ Checkpoint saved successfully")
logger.info(f"  Checkpoint path: {ckpt_path}")

# ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸
ckpt_path_obj = Path(ckpt_path)
if ckpt_path_obj.exists():
    total_size = sum(f.stat().st_size for f in ckpt_path_obj.rglob('*') if f.is_file())
    logger.info(f"  Total checkpoint size: {total_size / (1024*1024):.2f} MB")
    
    logger.info("  Checkpoint directory structure:")
    items = sorted(ckpt_path_obj.iterdir(), key=lambda x: (not x.is_dir(), x.name))
    for item in items[:15]:
        if item.is_dir():
            logger.info(f"    ğŸ“ {item.name}/")
        else:
            size_kb = item.stat().st_size / 1024
            logger.info(f"    ğŸ“„ {item.name} ({size_kb:.1f} KB)")
    if len(list(ckpt_path_obj.iterdir())) > 15:
        logger.info(f"    ... ({len(list(ckpt_path_obj.iterdir())) - 15} more items)")

logger.info("\nStopping the original algorithm...")
algo.stop()
logger.info("âœ“ Original algorithm stopped")

logger.info("\n" + "="*100)
logger.info("CHECKPOINT RESTORE OPERATION")
logger.info("="*100)

# ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° ë³µì›
# ì¤‘ìš”: ë³µì› ì‹œì—ë„ ë™ì¼í•œ env ë“±ë¡/creator, ì»¤ë„¥í„° import ê°€ ì‚´ì•„ ìˆì–´ì•¼ í•¨
logger.info(f"Restoring algorithm from checkpoint: {ckpt_path}")
logger.info("Calling Algorithm.from_checkpoint()...")

# from_checkpoint ëŠ” ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ëœ configë¥¼ ê·¸ëŒ€ë¡œ ë¡œë“œí•¨
algo_eval = Algorithm.from_checkpoint(ckpt_path)

logger.info("âœ“ Algorithm restored successfully from checkpoint")
logger.info(f"  Restored algorithm type: {type(algo_eval).__name__}")

logger.info("\n" + "="*100)
logger.info("EVALUATION OPERATION")
logger.info("="*100)

# í‰ê°€
logger.info("Starting evaluation with restored algorithm...")
eval_results = algo_eval.evaluate()
logger.info("âœ“ Evaluation completed")

# ê²°ê³¼ ì§‘ê³„
try:
    eval_data = eval_results.get("evaluation", {})
    if not eval_data:
        logger.warning("âš  No evaluation data found in results")
    else:
        eval_ep_ret_mean = eval_data.get("episode_return_mean", "N/A")
        eval_ep_len_mean = eval_data.get("episode_len_mean", "N/A")
        eval_num_episodes = eval_data.get("num_episodes", "N/A")
        
        logger.info("Evaluation Results:")
        logger.info(f"  Episode Return Mean: {eval_ep_ret_mean}")
        logger.info(f"  Episode Length Mean: {eval_ep_len_mean}")
        logger.info(f"  Number of Episodes: {eval_num_episodes}")
        
except KeyError as e:
    logger.warning(f"âš  Could not extract evaluation metrics (KeyError: {e})")
    logger.info("Full evaluation result structure:")
    pprint(eval_results)

# í‰ê°€ìš© ì•Œê³ ë¦¬ì¦˜ ì •ë¦¬
logger.info("\nStopping evaluation algorithm...")
algo_eval.stop()
logger.info("âœ“ Evaluation algorithm stopped")

logger.info("\n" + "="*100)
logger.info("SCRIPT COMPLETED SUCCESSFULLY")
logger.info("="*100)
logger.info("Summary:")
logger.info(f"  - Trained for 5 iterations")
logger.info(f"  - Saved checkpoint to: {ckpt_path}")
logger.info(f"  - Restored algorithm from checkpoint")
logger.info(f"  - Performed evaluation")
logger.info("="*100)
